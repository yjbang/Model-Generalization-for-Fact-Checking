{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Fake News\n",
    "#####\n",
    "class FakeNewsDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'fake': 0, 'real': 1}\n",
    "    INDEX2LABEL = {0: 'fake', 1: 'real'}\n",
    "    NUM_LABELS = 2\n",
    "    EMOJI_PATTERN = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\")\n",
    "\n",
    "    def preprocess_tweet(self, text):\n",
    "        text = re.sub(r'(https://\\S+)', '<URL>', text)\n",
    "        #     text = text.replace('THREAD: ', '')\n",
    "        text = self.EMOJI_PATTERN.sub(r'', text)\n",
    "        encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "        text = encoded_string.decode()\n",
    "        # text = text.replace('#', '')\n",
    "        text = text.replace('&amp;', '&')\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def load_dataset(self, path): \n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        if 'label' in df:\n",
    "            df['label'] = df['label'].apply(lambda x: self.LABEL2INDEX[x])\n",
    "#         if self.is_test:\n",
    "#             df = pd.DataFrame(columns=['id', 'tweet'])\n",
    "#             with open(path) as reader:\n",
    "#                 for l in reader.readlines()[1:]:\n",
    "#                     id, txt = l.split('\\t')\n",
    "#                     if self.process:\n",
    "#                         df = df.append({'id': id, 'tweet':self.preprocess_tweet(txt.strip())}, ignore_index=True)\n",
    "#                     else:\n",
    "#                         df = df.append({'id': id, 'tweet':txt.strip()}, ignore_index=True)\n",
    "#         else:\n",
    "#             df = pd.DataFrame(columns=['id', 'tweet', 'label'])\n",
    "#             with open(path) as reader:\n",
    "#                 for l in reader.readlines()[1:]:\n",
    "#                     id, txt, label = l.split('\\t')\n",
    "#                     if self.process:\n",
    "#                         df = df.append({'id': id, 'tweet':self.preprocess_tweet(txt.strip()), 'label':self.LABEL2INDEX[label.strip()]}, ignore_index=True)\n",
    "#                     else:\n",
    "#                         df = df.append({'id': id, 'tweet':txt.strip(), 'label':self.LABEL2INDEX[label.strip()]}, ignore_index=True)\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset_path=None, dataset=None, no_special_token=False, is_test=False, process=False, *args, **kwargs):\n",
    "        self.is_test = is_test\n",
    "        self.process = process\n",
    "        if dataset is not None:\n",
    "            self.data = dataset\n",
    "        else:\n",
    "            self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.no_special_token = no_special_token\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.loc[index,:]\n",
    "        if self.is_test:\n",
    "            id, text = index, data['tweet']\n",
    "            subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n",
    "            return id, np.array(subwords), data['tweet']\n",
    "        else:\n",
    "            id, text, label = index, data['tweet'], data['label']\n",
    "            subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n",
    "            return id, np.array(subwords), np.array(label), data['tweet']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "        \n",
    "class FakeNewsDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, is_test=False, *args, **kwargs):\n",
    "        super(FakeNewsDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[1]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        if not self.is_test:\n",
    "            label_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "        \n",
    "        ids = []\n",
    "        seq_list = []        \n",
    "        if self.is_test:\n",
    "            for i, (id, subwords, raw_seq) in enumerate(batch):\n",
    "                ids.append(id)\n",
    "                subwords = subwords[:max_seq_len]\n",
    "                subword_batch[i,:len(subwords)] = subwords\n",
    "                mask_batch[i,:len(subwords)] = 1\n",
    "                seq_list.append(raw_seq)\n",
    "\n",
    "            return ids, subword_batch, mask_batch, seq_list\n",
    "        else:\n",
    "            for i, (id, subwords, sentiment, raw_seq) in enumerate(batch):\n",
    "                ids.append(id)\n",
    "                subwords = subwords[:max_seq_len]\n",
    "                subword_batch[i,:len(subwords)] = subwords\n",
    "                mask_batch[i,:len(subwords)] = 1\n",
    "                label_batch[i,0] = sentiment\n",
    "\n",
    "                seq_list.append(raw_seq)\n",
    "\n",
    "            return ids, subword_batch, mask_batch, label_batch, seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pickle.load(open('./tmp/index_percent_list_all_old.pkl', 'rb'))\n",
    "for k, v in indices.items():\n",
    "    indices[k] = v + 1 # Increment index to get ID\n",
    "idx_99 = indices['0.99'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pickle.load(open('./tmp/index_percent_list_all_old.pkl', 'rb'))\n",
    "for k, v in indices.items():\n",
    "    indices[k] = v + 1 # Increment index to get ID\n",
    "idx_99 = indices['0.99'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_rand = [195, 3956, 2848, 4539, 5338, 885, 1524, 584, 3760, 1264, 359, 2630, 1493, 5419, 3402, 4038, 4139, 5896, 6176, 1390, 5526, 478, 1083, 1848, 4960, 3267, 5320, 2236, 4939, 3056, 5488, 618, 2804, 6007, 3726, 1796, 3740, 3357, 2980, 83, 198, 508, 2526, 3429, 5648, 58, 5594, 3689, 2703, 507, 3626, 3991, 5152, 1580, 2214, 2322, 3611, 2706, 834, 1692, 5383, 3494, 4748, 655, 4881, 120, 472, 747, 4811, 2413, 4212, 2065, 680, 5934, 1356, 3363, 612, 5576, 1851, 19, 4666, 1395, 3403, 5228, 5808, 1898, 4701, 1486, 3196, 1090, 3690, 2732, 1906, 366, 5694, 4674, 1752, 3559, 3438, 4661, 5028, 2024, 2818, 672, 232, 1006, 3703, 411, 2733, 3927, 604, 2189, 4277, 2426, 3780, 1969, 4607, 573, 1620, 4034, 4477, 77, 3964, 5482, 6381]\n",
    "idx_5_rand = [2244, 3158, 3868, 2832, 4122, 99, 6133, 2606, 2169, 5609, 1286, 5071, 5402, 1457 , 6387, 4565, 5707, 5686, 1155, 5225, 4998, 695, 4014, 2816, 4825, 1319, 895, 940 , 658, 4182, 204, 2706, 5820, 6239, 5506, 5915, 2466, 1162, 1297, 1633, 5906, 3851 , 6031, 5251, 5188, 2445, 4379, 2047, 2774, 509, 1414, 5776, 2076, 3442, 5722, 804 , 3736, 6416, 5026, 455, 6210, 1056, 6271, 3984, 5746, 6257, 4759, 3243, 1338, 1862 , 2317, 6229, 1839, 1761, 1890, 5236, 2880, 3159, 2283, 6360, 1647, 6191, 2180, 914 , 5025, 1824, 1424, 4451, 153, 5620, 4588, 5341, 295, 4835, 4404, 1137, 5034, 3096 , 5494, 616, 2519, 4526, 2098, 4773, 3958, 1807, 4791, 2686, 5877, 5193, 3481, 4553 , 2620, 5586, 5513, 6368, 4763, 240, 4853, 5721, 1946, 1677, 3622, 4222, 724, 4118 , 3106, 3416, 2839, 4502, 2294, 1908, 4620, 3372, 2522, 5566, 3585, 5202, 1786, 1105 , 4301, 6174, 1374, 3364, 6171, 116, 4181, 3565, 290, 4534, 1412, 2367, 4641, 6168 , 2602, 6009, 3675, 4591, 560, 38, 530, 4004, 5652, 3817, 1750, 1114, 4927, 2116 , 1092, 2130, 3914, 5731, 188, 6330, 6334, 457, 1216, 3600, 1208, 5072, 6269, 2729 , 2624, 1340, 1235, 832, 481, 4598, 2748, 1010, 1770, 2149, 1159, 463, 4728, 4947 , 2813, 5714, 382, 5051, 4472, 3707, 5114, 5647, 2874, 706, 2394, 5354, 5462, 441 , 1573, 919, 196, 4395, 3380, 911, 624, 3044, 1526, 3008, 1239, 852, 2072, 1599 , 1233, 2320, 3587, 6108, 4686, 4909, 2705, 5102, 6164, 2775, 6035, 2984, 1804, 4975 , 1453, 4512, 5789, 524, 5155, 3056, 1552, 3976, 4375, 5115, 1583, 5049, 941, 3760 , 5682, 512, 1516, 3388, 2357, 4538, 2426, 5298, 4577, 310, 6162, 469, 3135, 5004 , 3452, 2315, 2161, 3646, 1283, 165, 5235, 3058, 3854, 4770, 151, 3696, 3035, 6234 , 5218, 2203, 2408, 1910, 3877, 6032, 2194, 2494, 3552, 1256, 1521, 2770, 3936, 5784 , 1806, 1749, 1502, 3796, 219, 1611, 2727, 4227, 5252, 3987, 5641, 1000, 4722, 4255 , 5662, 183, 6127, 5815, 6088, 2087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/train.tsv'\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "dataset = FakeNewsDataset(dataset_path=dataset_path, tokenizer=tokenizer, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6299, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.data\n",
    "json.dump(df.loc[df['id'].isin(idx_2_rand),:].to_dict(orient='instance'), open('clean_2%_rand.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.data\n",
    "json.dump(df.loc[df['id'].isin(idx_5_rand),:].to_dict(orient='instance'), open('clean_5%_rand.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.data\n",
    "json.dump(df.loc[~df.index.isin(idx_99),:].to_dict(orient='instance'), open('clean_99%.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2 = df.loc[df['id'].isin(idx_2_rand),'id'].tolist()\n",
    "id5 = df.loc[df['id'].isin(idx_5_rand),'id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df.loc[~df.index.isin(idx_99),'id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(len(ids)-1):\n",
    "    if ids[i+1] == ids[i] + 1:\n",
    "        c+= 1\n",
    "c, len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 314, 125, 62, 62, 310, 121)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ids)), len(set(id5)), len(set(id2)), len(set(ids) - set(id5)), len(set(ids) - set(id2)), len(set(id5) - set(id2)), len(set(id2) - set(id5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/valid.tsv'\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "dataset = FakeNewsDataset(dataset_path=dataset_path, tokenizer=tokenizer, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/test.tsv'\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "dataset = FakeNewsDataset(dataset_path=dataset_path, tokenizer=tokenizer, lowercase=False, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
