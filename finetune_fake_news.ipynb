{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning FakeNewsAAAI\n",
    "FakeNewsAAAI is a Fake News dataset with 2 possible labels: `real` and `fake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "from utils.forward_fn import forward_mask_sequence_classification\n",
    "from utils.metrics import classification_metrics_fn\n",
    "from utils.data_utils import FakeNewsDataset, FakeNewsDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# common functions\n",
    "###\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.4f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(26092020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = AutoConfig.from_pretrained('roberta-base')\n",
    "config.num_labels = FakeNewsDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124647170"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './data/train.tsv'\n",
    "valid_dataset_path = './data/valid.tsv'\n",
    "# test_dataset_path = './dataset/test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FakeNewsDataset(dataset_path=train_dataset_path, tokenizer=tokenizer, lowercase=False)\n",
    "valid_dataset = FakeNewsDataset(dataset_path=valid_dataset_path, tokenizer=tokenizer, lowercase=False)\n",
    "# test_dataset = FakeNewsDataset(dataset_path=test_dataset_path, tokenizer=tokenizer, lowercase=False)\n",
    "\n",
    "train_loader = FakeNewsDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=8, num_workers=8, shuffle=True)  \n",
    "valid_loader = FakeNewsDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=8, num_workers=8, shuffle=False)  \n",
    "# test_loader = FakeNewsDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=8, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fake': 0, 'real': 1}\n",
      "{0: 'fake', 1: 'real'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = FakeNewsDataset.LABEL2INDEX, FakeNewsDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6581 LR:0.00000300:  25%|██▌       | 199/788 [00:21<01:13,  7.98it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1635 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 1) TRAIN LOSS:0.5224 LR:0.00000300:  44%|████▍     | 345/788 [00:37<00:47,  9.33it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4568 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 1) TRAIN LOSS:0.4702 LR:0.00000300:  53%|█████▎    | 415/788 [00:44<00:37, 10.06it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 1) TRAIN LOSS:0.3601 LR:0.00000300:  82%|████████▏ | 645/788 [01:08<00:14,  9.93it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1925 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 1) TRAIN LOSS:0.3179 LR:0.00000300: 100%|██████████| 788/788 [01:23<00:00,  9.45it/s]\n",
      "  0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.3179 ACC:0.8447 F1:0.8430 REC:0.8417 PRE:0.8500 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1472 ACC:0.9431 F1:0.9424 REC:0.9400 PRE:0.9492:  74%|███████▍  | 199/268 [00:07<00:02, 24.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "VALID LOSS:0.1544 ACC:0.9392 F1:0.9387 REC:0.9368 PRE:0.9451: 100%|██████████| 268/268 [00:10<00:00, 26.05it/s]\n",
      "  0%|          | 0/788 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.1544 ACC:0.9392 F1:0.9387 REC:0.9368 PRE:0.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.0758 LR:0.00000300:  36%|███▌      | 282/788 [00:29<00:50,  9.94it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 2) TRAIN LOSS:0.0845 LR:0.00000300:  44%|████▎     | 343/788 [00:36<00:47,  9.44it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1635 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 2) TRAIN LOSS:0.0906 LR:0.00000300:  85%|████████▌ | 673/788 [01:11<00:12,  9.05it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1925 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 2) TRAIN LOSS:0.0919 LR:0.00000300: 100%|██████████| 788/788 [01:23<00:00,  9.42it/s]\n",
      "  0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.0919 ACC:0.9695 F1:0.9694 REC:0.9693 PRE:0.9696 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0828 ACC:0.9744 F1:0.9743 REC:0.9738 PRE:0.9748:  74%|███████▍  | 199/268 [00:07<00:02, 25.08it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "VALID LOSS:0.0887 ACC:0.9710 F1:0.9709 REC:0.9705 PRE:0.9715: 100%|██████████| 268/268 [00:10<00:00, 26.35it/s]\n",
      "  0%|          | 0/788 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.0887 ACC:0.9710 F1:0.9709 REC:0.9705 PRE:0.9715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.0214 LR:0.00000300:  36%|███▌      | 283/788 [00:30<00:51,  9.90it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 3) TRAIN LOSS:0.0263 LR:0.00000300:  44%|████▎     | 343/788 [00:36<00:47,  9.42it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1635 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 3) TRAIN LOSS:0.0253 LR:0.00000300:  85%|████████▌ | 673/788 [01:11<00:12,  9.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1925 > 512). Running this sequence through the model will result in indexing errors\n",
      "(Epoch 3) TRAIN LOSS:0.0252 LR:0.00000300: 100%|██████████| 788/788 [01:24<00:00,  9.37it/s]\n",
      "  0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.0252 ACC:0.9936 F1:0.9936 REC:0.9936 PRE:0.9937 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0923 ACC:0.9750 F1:0.9749 REC:0.9743 PRE:0.9757:  74%|███████▍  | 198/268 [00:07<00:02, 25.02it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "VALID LOSS:0.0987 ACC:0.9715 F1:0.9714 REC:0.9708 PRE:0.9723: 100%|██████████| 268/268 [00:10<00:00, 26.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.0987 ACC:0.9715 F1:0.9714 REC:0.9708 PRE:0.9723\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        outputs = forward_mask_sequence_classification(model, batch_data[:-1], i2w=i2w, apply_mask=True, device='cuda')\n",
    "        loss, batch_hyp, batch_label, logits, label_batch = outputs\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = classification_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        outputs = forward_mask_sequence_classification(model, batch_data[:-1], i2w=i2w, apply_mask=True, device='cuda')\n",
    "        loss, batch_hyp, batch_label, logits, label_batch = outputs\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = classification_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = classification_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate on test\n",
    "# model.eval()\n",
    "# torch.set_grad_enabled(False)\n",
    "\n",
    "# total_loss, total_correct, total_labels = 0, 0, 0\n",
    "# list_hyp, list_label = [], []\n",
    "\n",
    "# pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "# for i, batch_data in enumerate(pbar):\n",
    "#     _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "#     list_hyp += batch_hyp\n",
    "\n",
    "# # Save prediction\n",
    "# df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "# df.index = df.index + 1\n",
    "# df.to_csv('prediction.csv')\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from utils.utils import generate_random_mask\n",
    "\n",
    "def influence_score(model, id, subword, mask, label, device='cpu'):\n",
    "    loss_fct = CrossEntropyLoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        # Prepare input & label\n",
    "        subword = torch.LongTensor(subword)\n",
    "        mask = torch.FloatTensor(mask)\n",
    "        label = torch.LongTensor(label)\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            subword = subword.cuda()\n",
    "            mask = mask.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "        if isinstance(model, BertForSequenceClassification):\n",
    "            # Apply mask\n",
    "            weight, bias = model.classifier.weight, model.classifier.bias\n",
    "            dropout_mask = generate_random_mask([id], weight.shape[0], weight.shape[1], device=device).repeat(subword.shape[0],1,1)\n",
    "            masked_weight = weight.expand_as(dropout_mask) * dropout_mask\n",
    "\n",
    "            # Calculate latents\n",
    "            latents = model.bert(subword, attention_mask=mask)[1]\n",
    "            latents = model.dropout(latents)            \n",
    "        elif isinstance(model, RobertaForSequenceClassification):\n",
    "            # Apply mask\n",
    "            weight, bias = model.classifier.out_proj.weight, model.classifier.out_proj.bias\n",
    "            dropout_mask = generate_random_mask([id], weight.shape[0], weight.shape[1], device=device).repeat(subword.shape[0],1,1)\n",
    "            masked_weight = weight.expand_as(dropout_mask) * dropout_mask\n",
    "\n",
    "            # Calculate latents\n",
    "            latents = model.roberta(subword, attention_mask=mask)[0][:,0,:]\n",
    "            latents = model.classifier.dense(latents)\n",
    "            latents = model.classifier.dropout(latents)\n",
    "        else:\n",
    "            ValueError(f'Model class `{type(model)}` is not implemented yet')\n",
    "\n",
    "        # Compute loss with mask\n",
    "        logits = torch.einsum('bd,bcd->bc', latents, masked_weight) + bias\n",
    "        mask_loss = loss_fct(logits.view(-1, model.num_labels), label.view(-1))\n",
    "\n",
    "        # Compute loss with flipped mask\n",
    "        logits = torch.einsum('bd,bcd->bc', latents, (masked_weight.max() - masked_weight)) + bias\n",
    "        flipped_mask_loss = loss_fct(logits.view(-1, model.num_labels), label.view(-1))\n",
    "                              \n",
    "        return flipped_mask_loss - mask_loss\n",
    "                              \n",
    "def build_influence_matrix(model, data_loader, train_size, device='cpu'):\n",
    "    test_size = len(data_loader.dataset)\n",
    "    influence_mat = torch.zeros(test_size, train_size, device=device)\n",
    "    \n",
    "    id2idx = {}\n",
    "    for i, batch_data in enumerate(data_loader):\n",
    "        print(f'Processing batch {i}/{len(data_loader)}')\n",
    "        (ids, subword_batch, mask_batch, label_batch, seq_list) = batch_data\n",
    "        token_type_batch = None\n",
    "\n",
    "        for train_idx in tqdm(range(train_size)):\n",
    "            train_id = train_idx + 1\n",
    "            scores = influence_score(model, train_id, subword_batch, mask_batch, label_batch, device=device)\n",
    "            for i, id in enumerate(ids):\n",
    "                id2idx[id] = i\n",
    "                influence_mat[i, train_idx] = scores[i]\n",
    "    return influence_mat, id2idx\n",
    "\n",
    "def get_inference_result(model, data_loader, device='cpu'):\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, leave=True, total=len(data_loader))\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            batch_id = batch_data[0]\n",
    "            batch_seq = batch_data[-1]\n",
    "            outputs = forward_mask_sequence_classification(model, batch_data[:-1], i2w=i2w, apply_mask=True, device='cuda')\n",
    "            loss, batch_hyp, batch_label, logits, label_batch = outputs\n",
    "\n",
    "            for i, id in enumerate(batch_id):\n",
    "                results[id] = batch_hyp[i] == batch_label[i]\n",
    "    return results\n",
    "\n",
    "def get_filtered_dataloader(data_loader, id_list, batch_size=8, shuffle=False):\n",
    "    df = data_loader.dataset.data\n",
    "    filt_df = df[df['id'].isin(id_list)].reset_index(drop=True)\n",
    "    dataset = FakeNewsDataset(dataset_path=None, dataset=filt_df, tokenizer=tokenizer, lowercase=False)\n",
    "    data_loader = FakeNewsDataLoader(dataset=dataset, max_seq_len=512, batch_size=batch_size, num_workers=8, shuffle=shuffle)  \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 ms, sys: 23.7 ms, total: 44.5 ms\n",
      "Wall time: 42.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.7643, 0.0693, 8.5639], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "(ids, subword_batch, mask_batch, label_batch, seq_list) = batch_data\n",
    "influence_score(model, ids[0], subword_batch, mask_batch, label_batch, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/268 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/268 [00:00<01:53,  2.35it/s]\u001b[A\n",
      "  2%|▏         | 5/268 [00:00<01:20,  3.27it/s]\u001b[A\n",
      "  3%|▎         | 8/268 [00:00<00:58,  4.46it/s]\u001b[A\n",
      "  4%|▍         | 12/268 [00:00<00:42,  6.04it/s]\u001b[A\n",
      "  6%|▌         | 16/268 [00:00<00:31,  8.06it/s]\u001b[A\n",
      "  7%|▋         | 20/268 [00:00<00:23, 10.58it/s]\u001b[A\n",
      "  9%|▉         | 24/268 [00:01<00:18, 13.49it/s]\u001b[A\n",
      " 10%|█         | 28/268 [00:01<00:14, 16.69it/s]\u001b[A\n",
      " 12%|█▏        | 32/268 [00:01<00:11, 19.99it/s]\u001b[A\n",
      " 13%|█▎        | 36/268 [00:01<00:09, 23.26it/s]\u001b[A\n",
      " 15%|█▌        | 41/268 [00:01<00:08, 26.86it/s]\u001b[A\n",
      " 17%|█▋        | 46/268 [00:01<00:07, 30.13it/s]\u001b[A\n",
      " 19%|█▉        | 51/268 [00:01<00:06, 33.24it/s]\u001b[A\n",
      " 21%|██        | 56/268 [00:01<00:05, 35.54it/s]\u001b[A\n",
      " 23%|██▎       | 61/268 [00:01<00:05, 36.90it/s]\u001b[A\n",
      " 25%|██▍       | 66/268 [00:02<00:05, 38.29it/s]\u001b[A\n",
      " 26%|██▋       | 71/268 [00:02<00:05, 38.40it/s]\u001b[A\n",
      " 28%|██▊       | 76/268 [00:02<00:05, 38.39it/s]\u001b[A\n",
      " 30%|███       | 81/268 [00:02<00:04, 38.99it/s]\u001b[A\n",
      " 32%|███▏      | 86/268 [00:02<00:04, 39.46it/s]\u001b[A\n",
      " 34%|███▍      | 91/268 [00:02<00:04, 39.24it/s]\u001b[A\n",
      " 36%|███▌      | 96/268 [00:02<00:04, 40.85it/s]\u001b[A\n",
      " 38%|███▊      | 101/268 [00:02<00:04, 39.39it/s]\u001b[A\n",
      " 39%|███▉      | 105/268 [00:03<00:04, 39.21it/s]\u001b[A\n",
      " 41%|████      | 110/268 [00:03<00:03, 40.06it/s]\u001b[A\n",
      " 43%|████▎     | 115/268 [00:03<00:03, 39.91it/s]\u001b[A\n",
      " 45%|████▍     | 120/268 [00:03<00:03, 40.15it/s]\u001b[A\n",
      " 47%|████▋     | 125/268 [00:03<00:03, 38.89it/s]\u001b[A\n",
      " 48%|████▊     | 129/268 [00:03<00:03, 39.05it/s]\u001b[A\n",
      " 50%|████▉     | 133/268 [00:03<00:03, 39.29it/s]\u001b[A\n",
      " 51%|█████▏    | 138/268 [00:03<00:03, 40.62it/s]\u001b[A\n",
      " 53%|█████▎    | 143/268 [00:04<00:03, 41.09it/s]\u001b[A\n",
      " 55%|█████▌    | 148/268 [00:04<00:02, 40.82it/s]\u001b[A\n",
      " 57%|█████▋    | 153/268 [00:04<00:02, 42.37it/s]\u001b[A\n",
      " 59%|█████▉    | 158/268 [00:04<00:02, 41.12it/s]\u001b[A\n",
      " 61%|██████    | 163/268 [00:04<00:02, 40.22it/s]\u001b[A\n",
      " 63%|██████▎   | 168/268 [00:04<00:02, 38.42it/s]\u001b[A\n",
      " 64%|██████▍   | 172/268 [00:04<00:02, 38.77it/s]\u001b[A\n",
      " 66%|██████▌   | 176/268 [00:04<00:02, 38.65it/s]\u001b[A\n",
      " 67%|██████▋   | 180/268 [00:04<00:02, 38.35it/s]\u001b[A\n",
      " 69%|██████▊   | 184/268 [00:05<00:02, 37.60it/s]\u001b[A\n",
      " 71%|███████   | 189/268 [00:05<00:02, 38.52it/s]\u001b[A\n",
      " 72%|███████▏  | 193/268 [00:05<00:01, 38.27it/s]\u001b[A\n",
      " 74%|███████▎  | 197/268 [00:05<00:01, 37.45it/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      " 75%|███████▌  | 201/268 [00:05<00:01, 37.93it/s]\u001b[A\n",
      " 76%|███████▋  | 205/268 [00:05<00:01, 37.45it/s]\u001b[A\n",
      " 78%|███████▊  | 209/268 [00:05<00:01, 36.98it/s]\u001b[A\n",
      " 79%|███████▉  | 213/268 [00:05<00:01, 36.71it/s]\u001b[A\n",
      " 81%|████████  | 217/268 [00:06<00:01, 28.96it/s]\u001b[A\n",
      " 82%|████████▏ | 221/268 [00:06<00:01, 31.51it/s]\u001b[A\n",
      " 84%|████████▍ | 225/268 [00:06<00:01, 33.62it/s]\u001b[A\n",
      " 85%|████████▌ | 229/268 [00:06<00:01, 35.11it/s]\u001b[A\n",
      " 87%|████████▋ | 234/268 [00:06<00:00, 37.32it/s]\u001b[A\n",
      " 89%|████████▉ | 239/268 [00:06<00:00, 38.02it/s]\u001b[A\n",
      " 91%|█████████ | 244/268 [00:06<00:00, 39.22it/s]\u001b[A\n",
      " 93%|█████████▎| 249/268 [00:06<00:00, 40.48it/s]\u001b[A\n",
      " 95%|█████████▍| 254/268 [00:06<00:00, 41.32it/s]\u001b[A\n",
      " 97%|█████████▋| 259/268 [00:07<00:00, 40.13it/s]\u001b[A\n",
      "100%|██████████| 268/268 [00:07<00:00, 36.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 999 ms, total: 7.35 s\n",
      "Wall time: 7.38 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = get_inference_result(model, valid_loader, device='cuda')\n",
    "correct_list = list(map(lambda kv: kv[0], filter(lambda kv: kv[1], results.items())))\n",
    "incorrect_list = list(map(lambda kv: kv[0], filter(lambda kv: not kv[1], results.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_valid_loader = get_filtered_dataloader(valid_loader, incorrect_list, batch_size=16)\n",
    "len(valid_loader), len(filt_valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 1/4 [04:27<13:21, 267.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 2/4 [09:31<09:16, 278.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 3/4 [14:40<04:47, 287.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "influence_matrix, id2idx = build_influence_matrix(model, filt_valid_loader, len(train_loader.dataset), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2139"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chinese converting to Islam after realising th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11 out of 13 people (from the Diamond Princess...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mike Pence in RNC speech praises Donald Trump’...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>2136</td>\n",
       "      <td>Donald Trump wrongly claimed that New Zealand ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2137</td>\n",
       "      <td>Current understanding is #COVID19 spreads most...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2138</td>\n",
       "      <td>Nothing screams “I am sat around doing fuck al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2139</td>\n",
       "      <td>Birx says COVID-19 outbreak not under control ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2140</td>\n",
       "      <td>Another 4422 new coronavirus cases have been c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2139 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  label\n",
       "0        1  Chinese converting to Islam after realising th...      0\n",
       "1        2  11 out of 13 people (from the Diamond Princess...      0\n",
       "2        3  COVID-19 Is Caused By A Bacterium, Not Virus A...      0\n",
       "3        4  Mike Pence in RNC speech praises Donald Trump’...      0\n",
       "4        5  6/10 Sky's @EdConwaySky explains the latest #C...      1\n",
       "...    ...                                                ...    ...\n",
       "2134  2136  Donald Trump wrongly claimed that New Zealand ...      0\n",
       "2135  2137  Current understanding is #COVID19 spreads most...      1\n",
       "2136  2138  Nothing screams “I am sat around doing fuck al...      0\n",
       "2137  2139  Birx says COVID-19 outbreak not under control ...      0\n",
       "2138  2140  Another 4422 new coronavirus cases have been c...      1\n",
       "\n",
       "[2139 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ df = valid_loader.dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine-tuned model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The CDC currently reports 99031 deaths. In general the discrepancies in death counts between different sources are small and explicable. The death toll stands at roughly 100000 people today. | Label : fake (99.921%)\n"
     ]
    }
   ],
   "source": [
    "text = 'The CDC currently reports 99031 deaths. In general the discrepancies in death counts between different sources are small and explicable. The death toll stands at roughly 100000 people today.'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Populous states can generate large case counts but if you look at the new cases per million today 9 smaller states are showing more cases per million than California or Texas: AL AR ID KS KY LA MS NV and SC. https://t.co/1pYW6cWRaS | Label : fake (99.958%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Populous states can generate large case counts but if you look at the new cases per million today 9 smaller states are showing more cases per million than California or Texas: AL AR ID KS KY LA MS NV and SC. https://t.co/1pYW6cWRaS'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Retraction—Hydroxychloroquine or chloroquine with or without a macrolide for treatment of COVID-19: a multinational registry analysis - The Lancet https://t.co/L5V2x6G9or | Label : real (97.656%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Retraction—Hydroxychloroquine or chloroquine with or without a macrolide for treatment of COVID-19: a multinational registry analysis - The Lancet https://t.co/L5V2x6G9or'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Chinese converting to Islam after realising that no muslim was affected by #Coronavirus #COVD19 in the country | Label : real (99.788%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Chinese converting to Islam after realising that no muslim was affected by #Coronavirus #COVD19 in the country'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
